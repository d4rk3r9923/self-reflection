{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI-API-KEY')\n",
    "\n",
    "intranet_azure_openai_api_key = os.getenv('INTRANET-AZURE-OPENAI-API-KEY')\n",
    "intranet_azure_openai_endpoint = os.getenv('INTRANET-AZURE-OPENAI-ENDPOINT')\n",
    "intranet_azure_openai_api_version = os.getenv('INTRANET-AZURE-OPENAI-API-VERSION')\n",
    "intranet_azure_openai_deployment = os.getenv('INTRANET-AZURE-OPENAI-DEPLOYMENT')\n",
    "intranet_azure_openai_model_name = os.getenv('INTRANET-AZURE-OPENAI-MODEL-NAME')\n",
    "\n",
    "intranet_azure_openai_embedding_api_key = os.getenv('INTRANET-AZURE-OPENAI-EMBEDDING-API-KEY')\n",
    "intranet_azure_openai_embedding_endpoint = os.getenv('INTRANET-AZURE-OPENAI-EMBEDDING-ENDPOINT')\n",
    "intranet_azure_openai_embedding_api_version = os.getenv('INTRANET-AZURE-OPENAI-EMBEDDING-API-VERSION')\n",
    "intranet_azure_openai_embedding_deployment_name = os.getenv('INTRANET-AZURE-OPENAI-EMBEDDING-DEPLOYMENT-NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "azure_openai_chat_model = AzureChatOpenAI(\n",
    "    model_name=intranet_azure_openai_model_name,\n",
    "    api_key=intranet_azure_openai_api_key,\n",
    "    azure_endpoint=intranet_azure_openai_endpoint,\n",
    "    api_version=intranet_azure_openai_api_version,\n",
    "    azure_deployment=intranet_azure_openai_deployment,\n",
    ")\n",
    "\n",
    "azure_openai_embeddings_model = AzureOpenAIEmbeddings(\n",
    "    api_key=intranet_azure_openai_embedding_api_key,\n",
    "    azure_endpoint=intranet_azure_openai_embedding_endpoint,\n",
    "    api_version=intranet_azure_openai_embedding_api_version,\n",
    "    azure_deployment=intranet_azure_openai_embedding_deployment_name,\n",
    ")\n",
    "\n",
    "openai_embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "pdf_directory = \"data/\"\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith(\".pdf\")]\n",
    "\n",
    "docs = [PyMuPDFLoader(pdf).load() for pdf in pdf_files]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=50)\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from uuid import uuid4\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"netflix\",\n",
    "    embedding_function=openai_embeddings_model,\n",
    "    persist_directory=\"netflix\",\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(doc_splits))]\n",
    "vector_store.add_documents(documents=doc_splits, ids=uuids)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Generator(BaseModel):\n",
    "    \"\"\"Tool used when the retrieved documents provide sufficient information to directly answer the human query.\n",
    "    The query field should contain the original question provided by the user.\n",
    "    The generation field will contain the answer to user generated by the model based on the retrieved documents if the documents are related to the query, else you will answer to the user why the query cannot be answered.\n",
    "    Please note that generation should keep internal state anonymous and not contain any personal information. Such as 'According to our knowledge,...'\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"The human question that can be directly answered by the retrieved documents.\",\n",
    "    )\n",
    "    generation: str = Field(\n",
    "        description=\"The answer generation of the query based on the documents. If there are no documents related or no documents returned, the generation will be why the query cannot be answered.\",\n",
    "    )\n",
    "\n",
    "class Inspector(BaseModel):\n",
    "    \"\"\"Tool used when the retrieved documents do not provide enough information or wrong information to answer the human query.\n",
    "    This tool helps to generate a new query based on related documents and feedback to retrieve more relevant information.\n",
    "    The query field should contain the expanded question.\n",
    "    The documents field will contain the list of documents that are related to the query and used to generate the new query.\n",
    "    The feedback field will contain the feedback on your own to evaluate if the new query is relevant to the original question and related docs for future invocation.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"The new expanded query generated from the related documents. The query will be used to search for more relevant documents to get more information for that query.\",\n",
    "    )\n",
    "    documents: list[str] = Field(\n",
    "        description=\"The list of document that are retrieved in closest relation to user query and used as foundation to generate the new query.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Your feedback to evaluation if the new query is relevant to the original question, what needs to be consider. Should be written from your POV, such as 'I think this should be...', 'I should look into...', 'The documents are correct but I think,...'\",\n",
    "    )\n",
    "\n",
    "llm_with_tools = azure_openai_chat_model.bind_tools([Generator, Inspector], tool_choice=\"any\")\n",
    "llm_with_generation_only_tool = azure_openai_chat_model.bind_tools([Generator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "<role>\n",
    "    - You are a Netflix supervisor that reflecting customer question by feedback and criticise user query and retrieved documents.\n",
    "    - You will be provided the documents that have been retrieved from the search function of Netflix Database to answer the human question.\n",
    "    - Your task is to evaluate the documents if the information enough and correct to answer the human question or we need to search for more relevant documents.\n",
    "    - If the documents are enough, use the Generator tool to generate the answer based on the documents for user, do not make up the answer.\n",
    "    - If the documents are not enough, use the Inspector tool to generate the expanded new query based on the related documents or such related to Netflix documents and feedback (if applicable) to retrieve more relevant information.\n",
    "    - If there are no documents returned, feedback to yourself that there are no documents to answer the human question and use the Inspector tool to generate the new query closely related to the original question.\n",
    "</role>\n",
    "\n",
    "<tools>\n",
    "    - Generator: use this tool in case the documents are enough to answer the human question, just input the original question as the query and answer generated from the documents, explain to user why the query cannot be answered if there are something wrong in retrieval process.\n",
    "    - Inspector: use this tool in case the documents are not enough to answer the human question, generate the new expanded query based on the related documents or query, domain should be related to Netflix or Netflix documents in database and feedback to retrieve more relevant information.\n",
    "</tools>\n",
    "\n",
    "<instruction>\n",
    "    - Evaluate the documents whether the information enough or not, more than a single document is sufficient enough to answer the human question. If not, we need to search for more relevant documents.\n",
    "    - Choose the tool that you want to use to continue the next step.\n",
    "    - If you use Inspector tool, the new query is generated only from the related documents if there are any else try to reassmble the query based on the original query, and feedback from your insights.\n",
    "    - Tools and feedback should be used to evaluate the information and generate the answer privately, user should not know details about these.\n",
    "</instruction>\n",
    "\n",
    "<feedback>\n",
    "    - Give out feedback to yourself whether you think the documents are enough to answer the human question or not, from your point of view.\n",
    "    - If you think the documents are not enough, feedback to yourself what needs to be consider to generate the new query.\n",
    "    - If you think the documents are enough, feedback to yourself that the documents are enough to answer the human question.\n",
    "    - Do not invent additional information; if you are uncertain about something, indicate that you do not know.\n",
    "</feedback>\n",
    "\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"\\\n",
    "Please answer the following question based on the documents provided by Netflix Database:\n",
    "{query}\n",
    "\n",
    "Here are the documents that have been retrieved from database of Netflix should be related to the user question:\n",
    "{documents}\n",
    "\n",
    "Here are your latest feedback to yourself, if there is an empty feedback, you should ignore it:\n",
    "{feedback}\n",
    "\n",
    "If you need more information, please let me know. Furthermore, if you think the documents are not enough to answer the question, please provide feedback on what needs to be consider to generate the new query.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does UK Tax Strategy can be applied in Italy or all of Europe in general?\"\n",
    "documents = [doc.page_content for doc in retriever.invoke(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from loguru import logger\n",
    "\n",
    "# Reflection loop after retrieval\n",
    "def process_query_with_expansions(query, documents, max_expansions=3, feedback=\"\"):\n",
    "    expansion_count = 0\n",
    "    current_query = query\n",
    "    current_feedback = feedback\n",
    "\n",
    "    while expansion_count < max_expansions:\n",
    "        response = llm_with_tools.invoke(\n",
    "            [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=human_prompt.format(\n",
    "                    query=current_query,\n",
    "                    documents=documents,\n",
    "                    feedback=current_feedback,\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if \"QueryExpansion\" in response.tool_calls[0].get('name'):\n",
    "            new_query = response.tool_calls[0].get('args').get('query')\n",
    "            new_feedback = response.tool_calls[0].get('args').get('feedback')\n",
    "            new_documents = response.tool_calls[0].get('args').get('documents', [])\n",
    "\n",
    "            current_query = new_query\n",
    "            current_feedback = new_feedback\n",
    "            documents = [doc for doc in new_documents]\n",
    "\n",
    "            expansion_count += 1\n",
    "\n",
    "            logger.info(response.tool_calls)\n",
    "        elif \"Generation\" in response.tool_calls[0].get('name'):\n",
    "            logger.info(response.tool_calls)\n",
    "            return response\n",
    "\n",
    "    response = llm_with_generation_only_tool.invoke(\n",
    "        [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=human_prompt.format(\n",
    "                query=current_query,\n",
    "                documents=documents,\n",
    "                feedback=current_feedback,\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    logger.info(response.tool_calls)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = process_query_with_expansions(query, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"You are an AI assistant that working in QA system to answer the human question. Based on the query and the docs, answer the human question with the best answer you can generate.\\n\\nHere are the documents that have been retrieved:\\n{documents}\"\"\"\n",
    "# human_prompt = \"The original question is: {query}\\n\\nPlease provide the best answer based on your internal documents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Does UK Tax Strategy can be applied in Italy?\"\n",
    "# documents = retriever.invoke(query)\n",
    "# docs = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = await azure_openai_chat_model.ainvoke(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=system_prompt.format(\n",
    "#                 documents=documents,\n",
    "#             )\n",
    "#         ),\n",
    "#         HumanMessage(\n",
    "#             content=human_prompt.format(\n",
    "#                 query=query,\n",
    "#             )\n",
    "#         )\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:\n",
    "- No hallucination / Stick to ground truth\n",
    "- Spelling\n",
    "- TODOS: memory?\n",
    "- Bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
